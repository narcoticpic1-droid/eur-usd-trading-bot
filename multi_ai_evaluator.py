# analyzers/multi_ai_evaluator.py
import asyncio
import datetime
import json
import sqlite3
import time
from typing import Dict, List, Optional, Tuple
import numpy as np

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAI library not installed")

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Google Generative AI library not installed")

try:
    import anthropic
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False
    print("âš ï¸ Anthropic library not installed")

import config

class MultiAIEvaluator:
    """
    Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¨Ø§ AI Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ
    """

    def __init__(self):
        self.db_path = "multi_crypto_analysis.db"
        self.ai_clients = {}
        self.ai_performance = {
            'gemini': {'correct': 0, 'total': 0, 'response_times': [], 'accuracy': 0.0},
            'openai': {'correct': 0, 'total': 0, 'response_times': [], 'accuracy': 0.0},
            'claude': {'correct': 0, 'total': 0, 'response_times': [], 'accuracy': 0.0}
        }
        
        # ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù¾ÙˆÛŒØ§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± AI
        self.ai_weights = {
            'gemini': 0.4,
            'openai': 0.35,
            'claude': 0.25
        }
        
        # Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÛŒÙÛŒØª
        self.quality_thresholds = {
            'high': 0.8,
            'medium': 0.6,
            'low': 0.4
        }
        
        self._setup_ai_clients()
        self._init_database()

    def _setup_ai_clients(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§ÛŒ AI"""
        print("Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ AI Clients...")
        
        # Gemini Setup
        if GEMINI_AVAILABLE and hasattr(config, 'GEMINI_API_KEY') and config.GEMINI_API_KEY:
            try:
                genai.configure(api_key=config.GEMINI_API_KEY)
                self.ai_clients['gemini'] = genai.GenerativeModel(
                    model_name=getattr(config, 'GEMINI_MODEL', 'gemini-pro')
                )
                print("âœ… Gemini AI connected")
            except Exception as e:
                print(f"âŒ Gemini setup failed: {e}")
        
        # OpenAI Setup
        if OPENAI_AVAILABLE and hasattr(config, 'OPENAI_API_KEY') and config.OPENAI_API_KEY:
            try:
                self.ai_clients['openai'] = openai.AsyncOpenAI(api_key=config.OPENAI_API_KEY)
                print("âœ… OpenAI connected")
            except Exception as e:
                print("âŒ OpenAI setup failed: {e}")
        
        # Claude Setup
        if CLAUDE_AVAILABLE and hasattr(config, 'CLAUDE_API_KEY') and config.CLAUDE_API_KEY:
            try:
                self.ai_clients['claude'] = anthropic.AsyncAnthropic(api_key=config.CLAUDE_API_KEY)
                print("âœ… Claude AI connected")
            except Exception as e:
                print(f"âŒ Claude setup failed: {e}")

    def _init_database(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS multi_ai_evaluations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    symbol TEXT NOT NULL,
                    signal_type TEXT,
                    original_confidence REAL,
                    gemini_score REAL,
                    gemini_recommendation TEXT,
                    openai_score REAL,
                    openai_recommendation TEXT,
                    claude_score REAL,
                    claude_recommendation TEXT,
                    consensus_score REAL,
                    final_recommendation TEXT,
                    execution_time REAL,
                    market_conditions TEXT
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ AI Ù‡Ø§
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ai_performance_tracking (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    ai_name TEXT,
                    signal_id TEXT,
                    prediction_score REAL,
                    actual_outcome REAL,
                    accuracy REAL,
                    response_time REAL
                )
            ''')
            
            conn.commit()
            conn.close()
            print("âœ… Multi-AI database initialized")
            
        except Exception as e:
            print(f"âŒ Database initialization failed: {e}")

    async def evaluate_signal(self, analysis_data: Dict, symbol: str) -> Optional[Dict]:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ø¨Ø§ Ú†Ù†Ø¯ÛŒÙ† AI"""
        start_time = time.time()
        
        try:
            if not analysis_data or analysis_data.get('signal', 0) == 0:
                return None
            
            print(f"ðŸ§  Starting multi-AI evaluation for {symbol}...")
            
            # Ø¯Ø±ÛŒØ§ÙØª Ù†Ø¸Ø±Ø§Øª Ù‡Ù…Ù‡ AI Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…Ø²Ù…Ø§Ù†
            ai_evaluations = await self._get_all_ai_evaluations(analysis_data, symbol)
            
            if not ai_evaluations:
                print(f"âŒ No AI evaluations received for {symbol}")
                return None
            
            # ØªØ­Ù„ÛŒÙ„ consensus
            consensus_analysis = self._analyze_consensus(ai_evaluations)
            
            # ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            final_decision = self._make_final_decision(analysis_data, ai_evaluations, consensus_analysis)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ execution time
            execution_time = time.time() - start_time
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
            self._save_evaluation_result(symbol, analysis_data, ai_evaluations, consensus_analysis, final_decision, execution_time)
            
            result = {
                'symbol': symbol,
                'original_analysis': analysis_data,
                'ai_evaluations': ai_evaluations,
                'consensus': consensus_analysis,
                'final_decision': final_decision,
                'execution_time': execution_time,
                'recommendation': final_decision.get('action', 'HOLD'),
                'confidence': final_decision.get('confidence', 0.0),
                'trade_worthy': final_decision.get('trade_worthy', False)
            }
            
            print(f"âœ… Multi-AI evaluation completed for {symbol} in {execution_time:.2f}s")
            return result
            
        except Exception as e:
            print(f"âŒ Multi-AI evaluation failed for {symbol}: {e}")
            return None

    async def _get_all_ai_evaluations(self, analysis_data: Dict, symbol: str) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø² Ù‡Ù…Ù‡ AI Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        tasks = []
        ai_names = []
        
        # Ø§ÛŒØ¬Ø§Ø¯ task Ø¨Ø±Ø§ÛŒ Ù‡Ø± AI Ù…ÙˆØ¬ÙˆØ¯
        for ai_name, client in self.ai_clients.items():
            if client:
                if ai_name == 'gemini':
                    tasks.append(self._evaluate_with_gemini(analysis_data, symbol))
                elif ai_name == 'openai':
                    tasks.append(self._evaluate_with_openai(analysis_data, symbol))
                elif ai_name == 'claude':
                    tasks.append(self._evaluate_with_claude(analysis_data, symbol))
                ai_names.append(ai_name)
        
        if not tasks:
            return {}
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ø¨Ø§ timeout
        try:
            results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=30.0
            )
            
            evaluations = {}
            for i, result in enumerate(results):
                if not isinstance(result, Exception) and result:
                    evaluations[ai_names[i]] = result
                elif isinstance(result, Exception):
                    print(f"âš ï¸ {ai_names[i]} evaluation failed: {result}")
            
            return evaluations
            
        except asyncio.TimeoutError:
            print("âš ï¸ AI evaluation timeout")
            return {}

    async def _evaluate_with_gemini(self, analysis_data: Dict, symbol: str) -> Optional[Dict]:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Gemini AI"""
        if 'gemini' not in self.ai_clients:
            return None
        
        try:
            prompt = self._create_evaluation_prompt(analysis_data, symbol, 'gemini')
            
            start_time = time.time()
            response = await self.ai_clients['gemini'].generate_content_async(prompt)
            response_time = time.time() - start_time
            
            self.ai_performance['gemini']['response_times'].append(response_time)
            
            evaluation = self._parse_ai_response(response.text, 'gemini')
            if evaluation:
                evaluation['response_time'] = response_time
            
            return evaluation
            
        except Exception as e:
            print(f"Gemini evaluation error: {e}")
            return None

    async def _evaluate_with_openai(self, analysis_data: Dict, symbol: str) -> Optional[Dict]:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ OpenAI"""
        if 'openai' not in self.ai_clients:
            return None
        
        try:
            prompt = self._create_evaluation_prompt(analysis_data, symbol, 'openai')
            
            start_time = time.time()
            response = await self.ai_clients['openai'].chat.completions.create(
                model=getattr(config, 'OPENAI_MODEL', 'gpt-4'),
                messages=[
                    {"role": "system", "content": self._get_system_prompt('openai')},
                    {"role": "user", "content": prompt}
                ],
                temperature=getattr(config, 'OPENAI_TEMPERATURE', 0.1),
                max_tokens=getattr(config, 'OPENAI_MAX_TOKENS', 1500)
            )
            response_time = time.time() - start_time
            
            self.ai_performance['openai']['response_times'].append(response_time)
            
            evaluation = self._parse_ai_response(response.choices[0].message.content, 'openai')
            if evaluation:
                evaluation['response_time'] = response_time
            
            return evaluation
            
        except Exception as e:
            print(f"OpenAI evaluation error: {e}")
            return None

    async def _evaluate_with_claude(self, analysis_data: Dict, symbol: str) -> Optional[Dict]:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Claude AI"""
        if 'claude' not in self.ai_clients:
            return None
        
        try:
            prompt = self._create_evaluation_prompt(analysis_data, symbol, 'claude')
            
            start_time = time.time()
            response = await self.ai_clients['claude'].messages.create(
                model=getattr(config, 'CLAUDE_MODEL', 'claude-3-5-sonnet-20240620'),
                max_tokens=getattr(config, 'CLAUDE_MAX_TOKENS', 1500),
                messages=[{"role": "user", "content": prompt}]
            )
            response_time = time.time() - start_time
            
            self.ai_performance['claude']['response_times'].append(response_time)
            
            evaluation = self._parse_ai_response(response.content[0].text, 'claude')
            if evaluation:
                evaluation['response_time'] = response_time
            
            return evaluation
            
        except Exception as e:
            print(f"Claude evaluation error: {e}")
            return None

    def _get_system_prompt(self, ai_type: str) -> str:
        """Ø³ÛŒØ³ØªÙ… prompt Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¹ AI"""
        base_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ ØªØ­Ù„ÛŒÙ„ ÙÙ†ÛŒ Ùˆ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø¯ÛŒØ¬ÛŒØªØ§Ù„ Ù‡Ø³ØªÛŒØ¯.

ØªØ®ØµØµâ€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§:
- ØªØ­Ù„ÛŒÙ„ Price Action Ùˆ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„
- Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ùˆ Ø³Ø§ÛŒØ²ÛŒÙ†Ú¯
- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ
- ØªØ­Ù„ÛŒÙ„ Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø±

Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø´Ù…Ø§:
- Ù…Ø­Ø§ÙØ¸Ù‡â€ŒÚ©Ø§Ø±Ø§Ù†Ù‡ Ùˆ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø¯Ø§Ø¯Ù‡
- Ø¯Ù‚ÛŒÙ‚ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø±ÛŒØ³Ú©/Ø±ÛŒÙˆØ§Ø±Ø¯
- ØµØ§Ø¯Ù‚ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§
- Ù…ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ø­ÙØ¸ Ø³Ø±Ù…Ø§ÛŒÙ‡"""

        ai_specializations = {
            'gemini': "\nØªØ®ØµØµ Ø®Ø§Øµ Ø´Ù…Ø§: ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„Ø§ØªÛŒ",
            'openai': "\nØªØ®ØµØµ Ø®Ø§Øµ Ø´Ù…Ø§: ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ±Ú©ÛŒØ¨ Ø¹ÙˆØ§Ù…Ù„ Ù…Ø®ØªÙ„Ù",
            'claude': "\nØªØ®ØµØµ Ø®Ø§Øµ Ø´Ù…Ø§: Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ùˆ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù…Ø­ØªØ§Ø·Ø§Ù†Ù‡"
        }
        
        return base_prompt + ai_specializations.get(ai_type, "")

    def _create_evaluation_prompt(self, analysis_data: Dict, symbol: str, ai_type: str) -> str:
        """Ø§ÛŒØ¬Ø§Ø¯ prompt Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ AI"""
        signal_direction = "Ø®Ø±ÛŒØ¯" if analysis_data['signal'] > 0 else "ÙØ±ÙˆØ´"
        signal_type = analysis_data.get('signal_type', 'Ù†Ø§Ù…Ø´Ø®Øµ')
        confidence = analysis_data.get('confidence', 0)
        current_price = analysis_data.get('current_price', 0)
        risk_reward = analysis_data.get('risk_reward_ratio', 0)
        
        prompt = f"""Ù„Ø·ÙØ§Ù‹ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯:

=== Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³ÛŒÚ¯Ù†Ø§Ù„ ===
Ù†Ù…Ø§Ø¯: {symbol}
Ù‚ÛŒÙ…Øª ÙØ¹Ù„ÛŒ: ${current_price:.4f}
Ø¬Ù‡Øª Ø³ÛŒÚ¯Ù†Ø§Ù„: {signal_direction}
Ù†ÙˆØ¹ Ø³ÛŒÚ¯Ù†Ø§Ù„: {signal_type}
Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø³ÛŒØ³ØªÙ…: {confidence:.2f}
Ù†Ø³Ø¨Øª Ø±ÛŒØ³Ú©/Ø±ÛŒÙˆØ§Ø±Ø¯: {risk_reward:.2f}

=== Ø¯Ù„Ø§ÛŒÙ„ Ø³ÛŒØ³ØªÙ… ==="""

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ù„Ø§ÛŒÙ„
        for reason in analysis_data.get('reasoning', []):
            prompt += f"\nâ€¢ {reason}"

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø±
        market_context = analysis_data.get('market_context', {})
        prompt += f"""

=== Ø´Ø±Ø§ÛŒØ· Ø¨Ø§Ø²Ø§Ø± ===
Ø³Ø§Ø®ØªØ§Ø±: {market_context.get('structure', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
Ù‚Ø¯Ø±Øª ØªØ±Ù†Ø¯: {market_context.get('trend_strength', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
ÙˆØ¶Ø¹ÛŒØª Ø­Ø¬Ù…: {market_context.get('volume_context', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
ADX: {market_context.get('adx', 'Ù†Ø§Ù…Ø´Ø®Øµ')}

=== Ø³Ø·ÙˆØ­ Ú©Ù„ÛŒØ¯ÛŒ ===
ÙˆØ±ÙˆØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ: ${analysis_data.get('entry_price', 0):.4f}
Ø­Ø¯ Ø¶Ø±Ø±: ${analysis_data.get('stop_loss', 0):.4f}
Ø§Ù‡Ø¯Ø§Ù Ø³ÙˆØ¯: {analysis_data.get('take_profits', [])}

=== Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ===
1. Ú©ÛŒÙÛŒØª setup Ø§Ø² Ù†Ø¸Ø± ÙÙ†ÛŒ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªØŸ
2. Ø¢ÛŒØ§ timing Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªØŸ
3. Ø±ÛŒØ³Ú©/Ø±ÛŒÙˆØ§Ø±Ø¯ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŸ
4. Ú†Ù‡ Ù†Ú¯Ø±Ø§Ù†ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
5. ØªÙˆØµÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ Ø´Ù…Ø§ Ú†ÛŒØ³ØªØŸ

Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¯Ø± Ù‚Ø§Ù„Ø¨ JSON Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯:
{{
    "technical_quality": 1-10,
    "timing_score": 1-10,
    "risk_assessment": "LOW/MEDIUM/HIGH/VERY_HIGH",
    "recommendation": "STRONG_BUY/BUY/HOLD/SELL/STRONG_SELL",
    "confidence": 0.0-1.0,
    "key_strengths": ["Ù‚ÙˆØª1", "Ù‚ÙˆØª2"],
    "main_concerns": ["Ù†Ú¯Ø±Ø§Ù†ÛŒ1", "Ù†Ú¯Ø±Ø§Ù†ÛŒ2"],
    "suggested_improvements": ["Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯1", "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯2"],
    "overall_score": 1-10
}}"""

        return prompt

    def _parse_ai_response(self, response_text: str, ai_name: str) -> Optional[Dict]:
        """ØªØ¬Ø²ÛŒÙ‡ Ù¾Ø§Ø³Ø® AI"""
        try:
            # ÛŒØ§ÙØªÙ† JSON Ø¯Ø± Ù¾Ø§Ø³Ø®
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            
            if start == -1 or end == 0:
                print(f"âš ï¸ No JSON found in {ai_name} response")
                return None
            
            json_str = response_text[start:end]
            parsed = json.loads(json_str)
            
            # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ ÙÛŒÙ„Ø¯Ù‡Ø§
            standardized = {
                'ai_name': ai_name,
                'technical_quality': parsed.get('technical_quality', 5),
                'timing_score': parsed.get('timing_score', 5),
                'risk_assessment': parsed.get('risk_assessment', 'MEDIUM'),
                'recommendation': parsed.get('recommendation', 'HOLD'),
                'confidence': float(parsed.get('confidence', 0.5)),
                'key_strengths': parsed.get('key_strengths', []),
                'main_concerns': parsed.get('main_concerns', []),
                'suggested_improvements': parsed.get('suggested_improvements', []),
                'overall_score': parsed.get('overall_score', 5),
                'raw_response': response_text[:500]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„
            }
            
            return standardized
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parse error for {ai_name}: {e}")
            return None
        except Exception as e:
            print(f"âŒ Response parsing error for {ai_name}: {e}")
            return None

    def _analyze_consensus(self, ai_evaluations: Dict) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ø¬Ù…Ø§Ø¹ Ø¨ÛŒÙ† AI Ù‡Ø§"""
        if not ai_evaluations:
            return {
                'type': 'NO_DATA',
                'strength': 0.0,
                'agreement_level': 'NONE',
                'conflicting_views': []
            }
        
        recommendations = []
        scores = []
        confidences = []
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        for ai_name, evaluation in ai_evaluations.items():
            recommendations.append(evaluation['recommendation'])
            scores.append(evaluation['overall_score'])
            confidences.append(evaluation['confidence'])
        
        # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ§ÙÙ‚
        unique_recommendations = list(set(recommendations))
        agreement_level = self._calculate_agreement_level(recommendations)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª consensus
        consensus_strength = self._calculate_consensus_strength(recommendations, scores, confidences)
        
        # ÛŒØ§ÙØªÙ† Ù†Ø¸Ø±Ø§Øª Ù…ØªØ¶Ø§Ø¯
        conflicting_views = self._identify_conflicts(ai_evaluations)
        
        # ØªØ¹ÛŒÛŒÙ† consensus type
        consensus_type = self._determine_consensus_type(unique_recommendations, agreement_level)
        
        return {
            'type': consensus_type,
            'strength': consensus_strength,
            'agreement_level': agreement_level,
            'unique_recommendations': unique_recommendations,
            'average_score': np.mean(scores),
            'average_confidence': np.mean(confidences),
            'conflicting_views': conflicting_views,
            'ai_count': len(ai_evaluations)
        }

    def _calculate_agreement_level(self, recommendations: List[str]) -> str:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø·Ø­ ØªÙˆØ§ÙÙ‚"""
        unique_count = len(set(recommendations))
        total_count = len(recommendations)
        
        if unique_count == 1:
            return 'FULL_AGREEMENT'
        elif unique_count == 2 and total_count >= 3:
            return 'MAJORITY_AGREEMENT'
        elif unique_count <= total_count * 0.6:
            return 'PARTIAL_AGREEMENT'
        else:
            return 'HIGH_DISAGREEMENT'

    def _calculate_consensus_strength(self, recommendations: List[str], scores: List[float], confidences: List[float]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‚Ø¯Ø±Øª consensus"""
        if not recommendations:
            return 0.0
        
        # ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø¬Ù…Ø§Ø¹
        most_common = max(set(recommendations), key=recommendations.count)
        agreement_ratio = recommendations.count(most_common) / len(recommendations)
        
        # ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©ÛŒÙÛŒØª
        avg_score = np.mean(scores) / 10.0  # normalize to 0-1
        avg_confidence = np.mean(confidences)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        consensus_strength = (agreement_ratio * 0.5) + (avg_score * 0.3) + (avg_confidence * 0.2)
        
        return min(consensus_strength, 1.0)

    def _identify_conflicts(self, ai_evaluations: Dict) -> List[Dict]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªØ¶Ø§Ø¯Ù‡Ø§"""
        conflicts = []
        
        ai_names = list(ai_evaluations.keys())
        for i in range(len(ai_names)):
            for j in range(i + 1, len(ai_names)):
                ai1, ai2 = ai_names[i], ai_names[j]
                eval1, eval2 = ai_evaluations[ai1], ai_evaluations[ai2]
                
                # Ø¨Ø±Ø±Ø³ÛŒ ØªØ¶Ø§Ø¯ Ø¯Ø± recommendation
                rec1, rec2 = eval1['recommendation'], eval2['recommendation']
                
                if self._are_conflicting_recommendations(rec1, rec2):
                    conflicts.append({
                        'ai1': ai1,
                        'ai2': ai2,
                        'ai1_recommendation': rec1,
                        'ai2_recommendation': rec2,
                        'ai1_confidence': eval1['confidence'],
                        'ai2_confidence': eval2['confidence'],
                        'conflict_type': 'RECOMMENDATION_CONFLICT'
                    })
        
        return conflicts

    def _are_conflicting_recommendations(self, rec1: str, rec2: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ØªØ¶Ø§Ø¯ Ø¨ÛŒÙ† ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§"""
        bullish = {'STRONG_BUY', 'BUY'}
        bearish = {'STRONG_SELL', 'SELL'}
        
        return (rec1 in bullish and rec2 in bearish) or (rec1 in bearish and rec2 in bullish)

    def _determine_consensus_type(self, unique_recommendations: List[str], agreement_level: str) -> str:
        """ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ consensus"""
        if agreement_level == 'FULL_AGREEMENT':
            return 'STRONG_CONSENSUS'
        elif agreement_level == 'MAJORITY_AGREEMENT':
            return 'MAJORITY_CONSENSUS'
        elif agreement_level == 'PARTIAL_AGREEMENT':
            return 'WEAK_CONSENSUS'
        else:
            return 'NO_CONSENSUS'

    def _make_final_decision(self, original_analysis: Dict, ai_evaluations: Dict, consensus: Dict) -> Dict:
        """ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ"""
        try:
            # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯
            if not ai_evaluations:
                return {
                    'action': 'HOLD',
                    'confidence': 0.0,
                    'trade_worthy': False,
                    'reason': 'No AI evaluations available'
                }
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² ÙˆØ²Ù†â€ŒØ¯Ø§Ø±
            weighted_score = 0.0
            total_weight = 0.0
            
            recommendations_mapping = {
                'STRONG_BUY': 2,
                'BUY': 1,
                'HOLD': 0,
                'SELL': -1,
                'STRONG_SELL': -2
            }
            
            for ai_name, evaluation in ai_evaluations.items():
                weight = self.ai_weights.get(ai_name, 0.33)
                score = recommendations_mapping.get(evaluation['recommendation'], 0)
                confidence = evaluation['confidence']
                
                weighted_score += score * confidence * weight
                total_weight += weight
            
            if total_weight > 0:
                final_score = weighted_score / total_weight
            else:
                final_score = 0
            
            # ØªØ¹ÛŒÛŒÙ† action Ù†Ù‡Ø§ÛŒÛŒ
            action = self._score_to_action(final_score)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ confidence Ù†Ù‡Ø§ÛŒÛŒ
            final_confidence = self._calculate_final_confidence(ai_evaluations, consensus, original_analysis)
            
            # Ø¨Ø±Ø±Ø³ÛŒ trade worthy
            trade_worthy = self._is_trade_worthy(action, final_confidence, consensus, original_analysis)
            
            return {
                'action': action,
                'confidence': final_confidence,
                'trade_worthy': trade_worthy,
                'weighted_score': final_score,
                'consensus_strength': consensus.get('strength', 0),
                'agreement_level': consensus.get('agreement_level', 'NONE'),
                'reason': self._generate_decision_reason(action, consensus, ai_evaluations),
                'risk_factors': self._identify_risk_factors(ai_evaluations),
                'supporting_factors': self._identify_supporting_factors(ai_evaluations)
            }
            
        except Exception as e:
            print(f"âŒ Final decision error: {e}")
            return {
                'action': 'HOLD',
                'confidence': 0.0,
                'trade_worthy': False,
                'reason': f'Decision error: {str(e)}'
            }

    def _score_to_action(self, score: float) -> str:
        """ØªØ¨Ø¯ÛŒÙ„ Ø§Ù…ØªÛŒØ§Ø² Ø¨Ù‡ action"""
        if score >= 1.5:
            return 'STRONG_BUY'
        elif score >= 0.5:
            return 'BUY'
        elif score <= -1.5:
            return 'STRONG_SELL'
        elif score <= -0.5:
            return 'SELL'
        else:
            return 'HOLD'

    def _calculate_final_confidence(self, ai_evaluations: Dict, consensus: Dict, original_analysis: Dict) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ confidence Ù†Ù‡Ø§ÛŒÛŒ"""
        try:
            # confidence Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø² AI Ù‡Ø§
            ai_confidences = [eval_data['confidence'] for eval_data in ai_evaluations.values()]
            avg_ai_confidence = np.mean(ai_confidences)
            
            # consensus strength
            consensus_strength = consensus.get('strength', 0)
            
            # confidence Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ…
            original_confidence = original_analysis.get('confidence', 0)
            
            # ØªØ±Ú©ÛŒØ¨ Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            final_confidence = (
                avg_ai_confidence * 0.4 +
                consensus_strength * 0.35 +
                original_confidence * 0.25
            )
            
            return min(final_confidence, 1.0)
            
        except Exception:
            return 0.5

    def _is_trade_worthy(self, action: str, confidence: float, consensus: Dict, original_analysis: Dict) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø±Ø²Ø´ Ù…Ø¹Ø§Ù…Ù„Ù‡"""
        # Ø´Ø±Ø§ÛŒØ· Ø§ØµÙ„ÛŒ
        min_confidence = getattr(config, 'SIGNAL_FILTERS', {}).get('min_confidence', 0.7)
        
        if action == 'HOLD':
            return False
        
        if confidence < min_confidence:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ consensus
        if consensus.get('agreement_level') in ['HIGH_DISAGREEMENT']:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø±ÛŒØ³Ú©/Ø±ÛŒÙˆØ§Ø±Ø¯
        min_risk_reward = getattr(config, 'SIGNAL_FILTERS', {}).get('min_risk_reward', 2.0)
        if original_analysis.get('risk_reward_ratio', 0) < min_risk_reward:
            return False
        
        return True

    def _generate_decision_reason(self, action: str, consensus: Dict, ai_evaluations: Dict) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ø¯Ù„ÛŒÙ„ ØªØµÙ…ÛŒÙ…"""
        agreement = consensus.get('agreement_level', 'NONE')
        ai_count = len(ai_evaluations)
        
        reason_parts = [
            f"Action: {action}",
            f"Based on {ai_count} AI evaluations",
            f"Agreement level: {agreement}",
            f"Consensus strength: {consensus.get('strength', 0):.2f}"
        ]
        
        return " | ".join(reason_parts)

    def _identify_risk_factors(self, ai_evaluations: Dict) -> List[str]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¹ÙˆØ§Ù…Ù„ Ø±ÛŒØ³Ú©"""
        risk_factors = []
        
        for ai_name, evaluation in ai_evaluations.items():
            for concern in evaluation.get('main_concerns', []):
                if concern not in risk_factors:
                    risk_factors.append(f"{ai_name}: {concern}")
        
        return risk_factors

    def _identify_supporting_factors(self, ai_evaluations: Dict) -> List[str]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¹ÙˆØ§Ù…Ù„ Ø­Ù…Ø§ÛŒØªâ€ŒÚ©Ù†Ù†Ø¯Ù‡"""
        supporting_factors = []
        
        for ai_name, evaluation in ai_evaluations.items():
            for strength in evaluation.get('key_strengths', []):
                if strength not in supporting_factors:
                    supporting_factors.append(f"{ai_name}: {strength}")
        
        return supporting_factors

    def _save_evaluation_result(self, symbol: str, analysis_data: Dict, ai_evaluations: Dict, 
                              consensus: Dict, final_decision: Dict, execution_time: float):
        """Ø°Ø®ÛŒØ±Ù‡ Ù†ØªÛŒØ¬Ù‡ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ØªÙ‡ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡
            gemini_data = ai_evaluations.get('gemini', {})
            openai_data = ai_evaluations.get('openai', {})
            claude_data = ai_evaluations.get('claude', {})
            
            cursor.execute('''
                INSERT INTO multi_ai_evaluations (
                    symbol, signal_type, original_confidence,
                    gemini_score, gemini_recommendation,
                    openai_score, openai_recommendation,
                    claude_score, claude_recommendation,
                    consensus_score, final_recommendation,
                    execution_time, market_conditions
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                symbol,
                analysis_data.get('signal_type', ''),
                analysis_data.get('confidence', 0),
                gemini_data.get('overall_score', 0),
                gemini_data.get('recommendation', ''),
                openai_data.get('overall_score', 0),
                openai_data.get('recommendation', ''),
                claude_data.get('overall_score', 0),
                claude_data.get('recommendation', ''),
                consensus.get('strength', 0),
                final_decision.get('action', ''),
                execution_time,
                json.dumps(analysis_data.get('market_context', {}))
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âŒ Save evaluation error: {e}")

    def update_ai_performance(self, ai_name: str, prediction_score: float, actual_outcome: float):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ AI"""
        try:
            if ai_name in self.ai_performance:
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ accuracy
                accuracy = 1.0 - abs(prediction_score - actual_outcome)
                
                # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
                self.ai_performance[ai_name]['total'] += 1
                if accuracy > 0.6:  # Ø¢Ø³ØªØ§Ù†Ù‡ Ù…ÙˆÙÙ‚ÛŒØª
                    self.ai_performance[ai_name]['correct'] += 1
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ accuracy Ú©Ù„ÛŒ
                total = self.ai_performance[ai_name]['total']
                correct = self.ai_performance[ai_name]['correct']
                self.ai_performance[ai_name]['accuracy'] = correct / total if total > 0 else 0
                
                # ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯
                self._adjust_ai_weights()
                
                print(f"ðŸ“Š {ai_name} performance updated: {self.ai_performance[ai_name]['accuracy']:.2%}")
        
        except Exception as e:
            print(f"âŒ Performance update error: {e}")

    def _adjust_ai_weights(self):
        """ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ AI Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        try:
            total_accuracy = 0
            active_ais = 0
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ accuracy Ú©Ù„
            for ai_name, stats in self.ai_performance.items():
                if stats['total'] > 5:  # Ø­Ø¯Ø§Ù‚Ù„ 5 Ù†Ù…ÙˆÙ†Ù‡
                    total_accuracy += stats['accuracy']
                    active_ais += 1
            
            if active_ais == 0:
                return
            
            # ØªÙ†Ø¸ÛŒÙ… ÙˆØ²Ù†â€ŒÙ‡Ø§
            for ai_name, stats in self.ai_performance.items():
                if stats['total'] > 5:
                    # ÙˆØ²Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø³Ø¨Øª accuracy Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
                    avg_accuracy = total_accuracy / active_ais
                    if avg_accuracy > 0:
                        weight_ratio = stats['accuracy'] / avg_accuracy
                        self.ai_weights[ai_name] = min(max(weight_ratio * 0.33, 0.1), 0.6)
            
            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
            total_weight = sum(self.ai_weights.values())
            if total_weight > 0:
                for ai_name in self.ai_weights:
                    self.ai_weights[ai_name] /= total_weight
                    
        except Exception as e:
            print(f"âŒ Weight adjustment error: {e}")

    def get_performance_report(self) -> Dict:
        """Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ AI Ù‡Ø§"""
        return {
            'ai_performance': self.ai_performance.copy(),
            'current_weights': self.ai_weights.copy(),
            'available_ais': list(self.ai_clients.keys()),
            'last_updated': datetime.datetime.now().isoformat()
        }
